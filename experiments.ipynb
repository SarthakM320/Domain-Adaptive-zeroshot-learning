{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ClipResnet\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "model = ClipResnet()\n",
    "\n",
    "ex = model(torch.randn(1,3,224,224).to(0),torch.randn(1,3,224,224).to(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 56, 56])\n",
      "torch.Size([1, 512, 28, 28])\n",
      "torch.Size([1, 1024, 14, 14])\n",
      "torch.Size([1, 2048, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "for x in ex:\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilinear_upsample(x, target_shape):\n",
    "    return F.interpolate(x, size = target_shape, mode = 'bilinear')\n",
    "\n",
    "upsampled = torch.cat([bilinear_upsample(i, ex[0].shape[-1]) for i in ex], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3840, 56, 56])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 224, 224])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            self._make_block(3840, 2048,3, padding=1, stride = 2),\n",
    "            self._make_block(2048, 1024,3, padding=1, stride = 2),\n",
    "            self._make_block(1024, 512,3,padding=1),\n",
    "            self._make_block(512,1,6,padding=1)\n",
    "        )\n",
    "\n",
    "    def _make_block(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=in_channels,out_channels=out_channels,kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace = True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "\n",
    "decoder = Decoder().to(0)\n",
    "# decoder(torch.rand(1,3840,7,7).to(0)).shape\n",
    "decoder(upsampled.to(torch.float32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "model, preprocess = clip.load('ViT-B/16', device = f'cuda:{0}')\n",
    "\n",
    "\n",
    "img = Image.open('test_image.png')\n",
    "img = preprocess(img).unsqueeze(0).to(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encode_image(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.visual(img.to(torch.float16)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "├─Conv2d: 1-1                                 [-1, 768, 14, 14]         589,824\n",
      "├─LayerNorm: 1-2                              [-1, 197, 768]            1,536\n",
      "├─Transformer: 1-3                            [-1, 2, 768]              --\n",
      "|    └─Sequential: 2-1                        [-1, 2, 768]              --\n",
      "|    |    └─ResidualAttentionBlock: 3-1       [-1, 2, 768]              7,087,872\n",
      "|    |    └─ResidualAttentionBlock: 3-2       [-1, 2, 768]              7,087,872\n",
      "|    |    └─ResidualAttentionBlock: 3-3       [-1, 2, 768]              7,087,872\n",
      "|    |    └─ResidualAttentionBlock: 3-4       [-1, 2, 768]              7,087,872\n",
      "|    |    └─ResidualAttentionBlock: 3-5       [-1, 2, 768]              7,087,872\n",
      "|    |    └─ResidualAttentionBlock: 3-6       [-1, 2, 768]              7,087,872\n",
      "|    |    └─ResidualAttentionBlock: 3-7       [-1, 2, 768]              7,087,872\n",
      "|    |    └─ResidualAttentionBlock: 3-8       [-1, 2, 768]              7,087,872\n",
      "|    |    └─ResidualAttentionBlock: 3-9       [-1, 2, 768]              7,087,872\n",
      "|    |    └─ResidualAttentionBlock: 3-10      [-1, 2, 768]              7,087,872\n",
      "|    |    └─ResidualAttentionBlock: 3-11      [-1, 2, 768]              7,087,872\n",
      "|    |    └─ResidualAttentionBlock: 3-12      [-1, 2, 768]              7,087,872\n",
      "├─LayerNorm: 1-4                              [-1, 768]                 1,536\n",
      "===============================================================================================\n",
      "Total params: 85,647,360\n",
      "Trainable params: 85,647,360\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 455.42\n",
      "===============================================================================================\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 2.59\n",
      "Params size (MB): 326.72\n",
      "Estimated Total Size (MB): 329.88\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "mod = model.visual.to(torch.float32)\n",
    "mod.proj = None\n",
    "summary(mod, (3,224,224));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feats = []\n",
    "def hook(module, input, output):\n",
    "    feats.append(output)\n",
    "\n",
    "# ids = [m[1].register_forward_hook(hook) for m in mod.named_modules()]\n",
    "\n",
    "id = mod.transformer.register_forward_hook(hook)\n",
    "_ = mod(torch.randn(2,3,224,224).to(0))\n",
    "id.remove()\n",
    "# for id in ids:\n",
    "#     id.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768, 14, 14])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = feats[0]\n",
    "\n",
    "out.transpose(0,1)[:,1:].transpose(1,2).reshape(2,768,14,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip \n",
    "import torch\n",
    "\n",
    "model, preprocess = clip.load('ViT-B/16', device = f'cuda:{0}')\n",
    "\n",
    "outs = model.visual(torch.rand(1,3,224,224).to(0).to(torch.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.forward(self, x: torch.Tensor)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.visual.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m outs\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "outs = model.visual.forward(torch.rand(1,3,224,224).to(0).to(torch.float16))\n",
    "outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encode_image(torch.rand(1,3,224,224).to(0).to(torch.float16)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 77])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(0)\n",
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encode_text(clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = []\n",
    "feats.append(model.encode_image(torch.rand(1,3,224,224).to(0).to(torch.float16)))\n",
    "feats.append(model.encode_text(clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing the forward function in Zegclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarth\\miniconda3\\envs\\zslearn\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512, 14, 14])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from models import CLIPViT\n",
    "import torch\n",
    "import math \n",
    "\n",
    "outs = CLIPViT().get_features(torch.rand(1,3,224,224).to(0).to(torch.float16),layers = [11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def d3_to_d4(t):\n",
    "    n, hw, c = t.size()\n",
    "    if hw % 2 != 0:\n",
    "        t = t[:, 1:]\n",
    "    h = w = int(math.sqrt(hw))\n",
    "    return t.transpose(1, 2).reshape(n, c, h, w)\n",
    "\n",
    "def d4_to_d3(t):\n",
    "    return t.flatten(-2).transpose(-1, -2)\n",
    "\n",
    "inputs_both = (outs, torch.rand(3,512).to(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([1, 512]), torch.Size([3, 512]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = inputs_both[0][0]\n",
    "cls_token = inputs_both[0][1]\n",
    "text_token = inputs_both[1]\n",
    "\n",
    "len(inputs), cls_token.shape, text_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 14, 14])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for stage_ in inputs[:1]:\n",
    "    x.append(d4_to_d3(stage_) if stage_.dim() > 3 else stage_)\n",
    "x.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1024])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_qs(q, cls):\n",
    "    # q = [q.cls, q]\n",
    "    C, dim = q.shape\n",
    "    bs, _ = cls.shape\n",
    "    q = q.expand(bs, -1, -1)\n",
    "    q1 = torch.einsum(\"bd,bcd->bcd\", cls, q)\n",
    "    q_ = torch.concat((q1, q), dim=-1)\n",
    "    return q_\n",
    "\n",
    "get_qs(text_token, cls_token).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_token.expand(1,-1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 14, 14])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarth\\miniconda3\\envs\\zslearn\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from models import SegDecoder\n",
    "import torch \n",
    "\n",
    "inputs_both = [\n",
    "    [(torch.rand(1,768,14,14).to(0),torch.rand(1,512,14,14).to(0)),torch.rand(1,512).to(0)],\n",
    "    torch.rand(3,512).to(0)\n",
    "]\n",
    "\n",
    "decoder = SegDecoder(224,768,[0,1],[0,1]).to(0)\n",
    "\n",
    "decoder(inputs_both)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zslearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
