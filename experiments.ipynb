{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "model, preprocess = clip.load('ViT-B/16', device = f'cuda:{0}')\n",
    "\n",
    "\n",
    "img = Image.open('test_image.png')\n",
    "img = preprocess(img).unsqueeze(0).to(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visual(torch.rand(1,3,224,224).to(0).to(torch.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encode_image(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visual(img.to(torch.float16)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "mod = model.visual.to(torch.float32)\n",
    "mod.proj = None\n",
    "summary(mod, (3,224,224));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feats = []\n",
    "def hook(module, input, output):\n",
    "    feats.append(output)\n",
    "\n",
    "# ids = [m[1].register_forward_hook(hook) for m in mod.named_modules()]\n",
    "\n",
    "id = mod.transformer.register_forward_hook(hook)\n",
    "_ = mod(torch.randn(2,3,224,224).to(0))\n",
    "id.remove()\n",
    "# for id in ids:\n",
    "#     id.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = feats[0]\n",
    "\n",
    "out.transpose(0,1)[:,1:].transpose(1,2).reshape(2,768,14,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip \n",
    "import torch\n",
    "\n",
    "model, preprocess = clip.load('ViT-B/16', device = f'cuda:{0}')\n",
    "\n",
    "outs = model.visual(torch.rand(1,3,224,224).to(0).to(torch.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visual.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = model.visual.forward(torch.rand(1,3,224,224).to(0).to(torch.float16))\n",
    "outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encode_image(torch.rand(1,3,224,224).to(0).to(torch.float16)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(0)\n",
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encode_text(clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = []\n",
    "feats.append(model.encode_image(torch.rand(1,3,224,224).to(0).to(torch.float16)))\n",
    "feats.append(model.encode_text(clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing the forward function in Zegclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import CLIPViT\n",
    "import torch\n",
    "import math \n",
    "\n",
    "outs = CLIPViT().get_features(torch.rand(1,3,224,224).to(0).to(torch.float16),layers = [11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def d3_to_d4(t):\n",
    "    n, hw, c = t.size()\n",
    "    if hw % 2 != 0:\n",
    "        t = t[:, 1:]\n",
    "    h = w = int(math.sqrt(hw))\n",
    "    return t.transpose(1, 2).reshape(n, c, h, w)\n",
    "\n",
    "def d4_to_d3(t):\n",
    "    return t.flatten(-2).transpose(-1, -2)\n",
    "\n",
    "inputs_both = (outs, torch.rand(3,512).to(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs_both[0][0]\n",
    "cls_token = inputs_both[0][1]\n",
    "text_token = inputs_both[1]\n",
    "\n",
    "len(inputs), cls_token.shape, text_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for stage_ in inputs[:1]:\n",
    "    x.append(d4_to_d3(stage_) if stage_.dim() > 3 else stage_)\n",
    "x.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qs(q, cls):\n",
    "    # q = [q.cls, q]\n",
    "    C, dim = q.shape\n",
    "    bs, _ = cls.shape\n",
    "    q = q.expand(bs, -1, -1)\n",
    "    q1 = torch.einsum(\"bd,bcd->bcd\", cls, q)\n",
    "    q_ = torch.concat((q1, q), dim=-1)\n",
    "    return q_\n",
    "\n",
    "get_qs(text_token, cls_token).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_token.expand(1,-1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import SegDecoder\n",
    "import torch \n",
    "\n",
    "inputs_both = [\n",
    "    [(torch.rand(1,768,14,14).to(0),torch.rand(1,512,14,14).to(0)),torch.rand(1,512).to(0)],\n",
    "    torch.rand(3,512).to(0)\n",
    "]\n",
    "\n",
    "decoder = SegDecoder(224,768,[0,1],[0,1]).to(0)\n",
    "\n",
    "decoder(inputs_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "\n",
    "image_encoder = clip.load('RN50', device = f'cuda:{0}')[0].visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = dict(image_encoder.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import CLIPResnet\n",
    "import torch\n",
    "\n",
    "feats = CLIPResnet().get_features(torch.rand(1,3,224,224).to(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in feats:\n",
    "    print(f.shape)\n",
    "\n",
    "feats.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Decoder\n",
    "\n",
    "decoder = Decoder(10).to(0).to(torch.float16)\n",
    "\n",
    "decoder(feats).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import CLIPResNet, CLIPVisionTransformer\n",
    "import torch \n",
    "\n",
    "model = CLIPResNet(\n",
    "    layers=[3, 4, 6, 3],\n",
    "    output_dim=1024,\n",
    "    heads = 8,\n",
    "    input_resolution=512\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = model.get_features(torch.rand(1,3,512,512))\n",
    "\n",
    "for o in outs:\n",
    "    print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import CLIPVisionTransformer\n",
    "import torch \n",
    "\n",
    "model = CLIPVisionTransformer(512,16,out_indices=[11,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = model(torch.rand(1,3,512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('trial_config.yaml', 'r') as f:\n",
    "    file = yaml.safe_load(f)\n",
    "\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import cityscapes_dataset_train\n",
    "\n",
    "img, target = cityscapes_dataset_train.__getitem__(10)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.imshow(img.permute(1,2,0).numpy())\n",
    "plt.show()\n",
    "plt.imshow(target.permute(1,2,0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "\n",
    "source = r'cityscapes_dataset'\n",
    "folders = ['gtFine','leftImg8bit', 'leftImg8bit_foggy']\n",
    "subset = 'train'\n",
    "\n",
    "frames = []\n",
    "for place_folder in os.listdir(join(source, folders[1], subset)):\n",
    "    for file in os.listdir(join(source, folders[1], subset, place_folder)):\n",
    "        place_name = place_folder\n",
    "        image_number = '_'.join(file.split('_')[1:3])\n",
    "        frames.append(pd.DataFrame(\n",
    "            {\n",
    "                'image': join(source, folders[1], subset, place_folder,file),\n",
    "                'foggy_image': join(source, folders[2], subset, place_folder,f'{place_name}_{image_number}_leftImg8bit_foggy_beta_0.02.png'),\n",
    "                'target':join(source, folders[0], subset, place_folder, f'{place_name}_{image_number}_gtFine_color.png')\n",
    "            },\n",
    "            index=[0]\n",
    "        ))\n",
    "\n",
    "df = pd.concat(frames).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "for i in range(len(df)):\n",
    "    images = df.iloc[i]\n",
    "    try:\n",
    "        im = Image.open(images.image)\n",
    "        im.verify()\n",
    "        im.close()\n",
    "    except:\n",
    "        print(images.image)\n",
    "    try:\n",
    "        im = Image.open(images.foggy_image)\n",
    "        im.verify()\n",
    "        im.close()\n",
    "    except:\n",
    "        print(images.foggy_image)\n",
    "    try:\n",
    "        im = Image.open(images.target)\n",
    "        im.verify()\n",
    "        im.close()\n",
    "    except:\n",
    "        print(images.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'dataset/cityscape_{subset}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import CLIPResNet, CLIPVisionTransformer\n",
    "import yaml\n",
    "import torch\n",
    "args = yaml.safe_load(open('trial_config.yaml', 'r'))\n",
    "\n",
    "resnet = CLIPResNet(\n",
    "        layers = args['Resnet']['layers'],\n",
    "        output_dim = args['Resnet']['output_dim'],\n",
    "        heads = args['Resnet']['heads'],\n",
    "        input_resolution = args['img_size'],\n",
    "        width = args['Resnet']['width'],\n",
    "        pretrained = args['Resnet']['pretrained']\n",
    "    )\n",
    "\n",
    "vit = CLIPVisionTransformer(\n",
    "        input_resolution = args['img_size'],\n",
    "        patch_size = args['VIT']['patch_size'],\n",
    "        width = args['VIT']['width'],\n",
    "        layers = args['VIT']['layers'],\n",
    "        heads = args['VIT']['heads'],\n",
    "        output_dim = args['VIT']['output_dim'],\n",
    "        drop_path_rate = args['VIT']['drop_path_rate'],\n",
    "        out_indices = args['VIT']['out_indices'],\n",
    "        get_embeddings = args['VIT']['get_embeddings'],\n",
    "        pretrained = args['VIT']['pretrained']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = resnet.get_features(torch.rand(1,3,512,512))\n",
    "for o in outs:\n",
    "    print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_outs = vit(torch.rand(1,3,512,512))\n",
    "vit_outs[0][0].shape, vit_outs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding out the classes in the cityscapes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import dataset, labels\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data = dataset('dataset/cityscape_train.csv', kaggle = False)\n",
    "\n",
    "image, foggy, gt = data.__getitem__(0)\n",
    "\n",
    "\n",
    "# gt = gt.permute(1,2,0)[:,:,:3]\n",
    "# gt_255 = (gt*255).to(torch.int)\n",
    "plt.imshow(image.permute(1,2,0))\n",
    "# plt.imshow(gt)\n",
    "# plt.show()\n",
    "# plt.imshow(gt_255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(34):\n",
    "    plt.imshow(gt[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(34,512,512)\n",
    "error = 0\n",
    "for i in range(512):\n",
    "    for j in range(512):\n",
    "        try:\n",
    "            color = tuple(gt[i,j].numpy())\n",
    "            id = color_mapping[color]\n",
    "            x[id,i,j] = 255\n",
    "        except:\n",
    "            x[0,i,j] = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0009)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "x1 = torch.randn(1,3,512,512).reshape(1,3,-1)\n",
    "x2 = torch.randn(1,3,512,512).reshape(1,3,-1)\n",
    "\n",
    "\n",
    "cos1 = nn.CosineSimilarity(dim=-1)\n",
    "(1-cos1(x1,x2)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 262144])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.randn(1,3,512,512)\n",
    "\n",
    "x1.flatten(-2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0008), tensor(-0.0009), tensor(-0.0002))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos1(x1,x2).mean(),cos2(x1,x2).mean(),cos3(x1,x2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0012600875"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import dataset, labels\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data = dataset('dataset/cityscape_train.csv', image_size = 32, color_mapping = {label.color:label.id for label in labels}, kaggle = False)\n",
    "\n",
    "image, foggy, gt = data.__getitem__(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.BCELoss()(gt,gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5003)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "label = torch.randint(low = 0, high = 2, size = (1,26,512,512))\n",
    "pred = torch.randint(low = 0, high = 2, size = (1,26,512,512))\n",
    "\n",
    "intersect = (pred & label).float().sum((2,3))\n",
    "union = (pred | label).float().sum((2,3))\n",
    "total_pixel_pred = pred.sum((2,3))\n",
    "total_pixel_truth = label.sum((2,3))\n",
    "\n",
    "(intersect/total_pixel_pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersect = (pred & label).float().sum((2,3))\n",
    "sum = pred.sum((2,3)) + label.sum((2,3))\n",
    "dice = 2*intersect/sum \n",
    "dice>intersect/union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 1.,  ..., 0., 1., 0.],\n",
       "          [1., 0., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 0.,  ..., 0., 1., 1.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 0.,  ..., 1., 1., 0.],\n",
       "          [0., 1., 1.,  ..., 0., 1., 0.]],\n",
       "\n",
       "         [[1., 1., 1.,  ..., 0., 1., 0.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [0., 1., 0.,  ..., 1., 1., 0.],\n",
       "          ...,\n",
       "          [1., 0., 0.,  ..., 1., 0., 0.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 1., 1.,  ..., 1., 0., 0.],\n",
       "          [0., 1., 1.,  ..., 1., 0., 0.],\n",
       "          [1., 1., 1.,  ..., 0., 0., 1.],\n",
       "          ...,\n",
       "          [0., 1., 0.,  ..., 1., 1., 0.],\n",
       "          [0., 1., 1.,  ..., 1., 0., 0.],\n",
       "          [0., 1., 1.,  ..., 1., 0., 1.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [1., 0., 0.,  ..., 1., 1., 0.],\n",
       "          [1., 0., 1.,  ..., 0., 1., 1.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 1., 1.],\n",
       "          [1., 0., 0.,  ..., 1., 0., 1.],\n",
       "          [0., 0., 0.,  ..., 0., 1., 0.]],\n",
       "\n",
       "         [[0., 0., 1.,  ..., 1., 1., 0.],\n",
       "          [0., 1., 1.,  ..., 0., 1., 0.],\n",
       "          [1., 0., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 0., 1., 0.],\n",
       "          [0., 0., 0.,  ..., 1., 0., 1.],\n",
       "          [1., 0., 0.,  ..., 0., 1., 1.]],\n",
       "\n",
       "         [[0., 1., 0.,  ..., 1., 0., 1.],\n",
       "          [1., 1., 0.,  ..., 1., 0., 1.],\n",
       "          [0., 0., 1.,  ..., 1., 0., 0.],\n",
       "          ...,\n",
       "          [1., 0., 0.,  ..., 0., 1., 1.],\n",
       "          [0., 1., 1.,  ..., 1., 0., 1.],\n",
       "          [0., 1., 1.,  ..., 1., 1., 0.]]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.rand(1,26,512,512)>0.5).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 2048, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch \n",
    "\n",
    "df = pd.read_csv('/Users/sarthakm/Desktop/biplabsir_research/Domain Adaptive ZeroShot Learning/dataset/cityscape_train.csv')\n",
    "\n",
    "gt = df['target'].values\n",
    "for image in gt:\n",
    "    im = np.array(Image.open(image.replace('\\\\','/')))[:,:,:3]\n",
    "    break\n",
    "\n",
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
