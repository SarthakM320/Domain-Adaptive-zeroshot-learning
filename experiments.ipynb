{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "model, preprocess = clip.load('ViT-B/16', device = f'cuda:{0}')\n",
    "\n",
    "\n",
    "img = Image.open('test_image.png')\n",
    "img = preprocess(img).unsqueeze(0).to(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visual(torch.rand(1,3,224,224).to(0).to(torch.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encode_image(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visual(img.to(torch.float16)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "mod = model.visual.to(torch.float32)\n",
    "mod.proj = None\n",
    "summary(mod, (3,224,224));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feats = []\n",
    "def hook(module, input, output):\n",
    "    feats.append(output)\n",
    "\n",
    "# ids = [m[1].register_forward_hook(hook) for m in mod.named_modules()]\n",
    "\n",
    "id = mod.transformer.register_forward_hook(hook)\n",
    "_ = mod(torch.randn(2,3,224,224).to(0))\n",
    "id.remove()\n",
    "# for id in ids:\n",
    "#     id.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = feats[0]\n",
    "\n",
    "out.transpose(0,1)[:,1:].transpose(1,2).reshape(2,768,14,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip \n",
    "import torch\n",
    "\n",
    "model, preprocess = clip.load('ViT-B/16', device = f'cuda:{0}')\n",
    "\n",
    "outs = model.visual(torch.rand(1,3,224,224).to(0).to(torch.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visual.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = model.visual.forward(torch.rand(1,3,224,224).to(0).to(torch.float16))\n",
    "outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encode_image(torch.rand(1,3,224,224).to(0).to(torch.float16)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(0)\n",
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encode_text(clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = []\n",
    "feats.append(model.encode_image(torch.rand(1,3,224,224).to(0).to(torch.float16)))\n",
    "feats.append(model.encode_text(clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing the forward function in Zegclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import CLIPViT\n",
    "import torch\n",
    "import math \n",
    "\n",
    "outs = CLIPViT().get_features(torch.rand(1,3,224,224).to(0).to(torch.float16),layers = [11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def d3_to_d4(t):\n",
    "    n, hw, c = t.size()\n",
    "    if hw % 2 != 0:\n",
    "        t = t[:, 1:]\n",
    "    h = w = int(math.sqrt(hw))\n",
    "    return t.transpose(1, 2).reshape(n, c, h, w)\n",
    "\n",
    "def d4_to_d3(t):\n",
    "    return t.flatten(-2).transpose(-1, -2)\n",
    "\n",
    "inputs_both = (outs, torch.rand(3,512).to(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs_both[0][0]\n",
    "cls_token = inputs_both[0][1]\n",
    "text_token = inputs_both[1]\n",
    "\n",
    "len(inputs), cls_token.shape, text_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for stage_ in inputs[:1]:\n",
    "    x.append(d4_to_d3(stage_) if stage_.dim() > 3 else stage_)\n",
    "x.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qs(q, cls):\n",
    "    # q = [q.cls, q]\n",
    "    C, dim = q.shape\n",
    "    bs, _ = cls.shape\n",
    "    q = q.expand(bs, -1, -1)\n",
    "    q1 = torch.einsum(\"bd,bcd->bcd\", cls, q)\n",
    "    q_ = torch.concat((q1, q), dim=-1)\n",
    "    return q_\n",
    "\n",
    "get_qs(text_token, cls_token).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_token.expand(1,-1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import SegDecoder\n",
    "import torch \n",
    "\n",
    "inputs_both = [\n",
    "    [(torch.rand(1,768,14,14).to(0),torch.rand(1,512,14,14).to(0)),torch.rand(1,512).to(0)],\n",
    "    torch.rand(3,512).to(0)\n",
    "]\n",
    "\n",
    "decoder = SegDecoder(224,768,[0,1],[0,1]).to(0)\n",
    "\n",
    "decoder(inputs_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "\n",
    "image_encoder = clip.load('RN50', device = f'cuda:{0}')[0].visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = dict(image_encoder.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import CLIPResnet\n",
    "import torch\n",
    "\n",
    "feats = CLIPResnet().get_features(torch.rand(1,3,224,224).to(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in feats:\n",
    "    print(f.shape)\n",
    "\n",
    "feats.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Decoder\n",
    "\n",
    "decoder = Decoder(10).to(0).to(torch.float16)\n",
    "\n",
    "decoder(feats).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import CLIPResNet, CLIPVisionTransformer\n",
    "import torch \n",
    "\n",
    "model = CLIPResNet(\n",
    "    layers=[3, 4, 6, 3],\n",
    "    output_dim=1024,\n",
    "    heads = 8,\n",
    "    input_resolution=512\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = model.get_features(torch.rand(1,3,512,512))\n",
    "\n",
    "for o in outs:\n",
    "    print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import CLIPVisionTransformer\n",
    "import torch \n",
    "\n",
    "model = CLIPVisionTransformer(512,16,out_indices=[11,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = model(torch.rand(1,3,512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('trial_config.yaml', 'r') as f:\n",
    "    file = yaml.safe_load(f)\n",
    "\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import cityscapes_dataset_train\n",
    "\n",
    "img, target = cityscapes_dataset_train.__getitem__(10)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.imshow(img.permute(1,2,0).numpy())\n",
    "plt.show()\n",
    "plt.imshow(target.permute(1,2,0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "\n",
    "source = r'cityscapes_dataset'\n",
    "folders = ['gtFine','leftImg8bit', 'leftImg8bit_foggy']\n",
    "subset = 'val'\n",
    "\n",
    "frames = []\n",
    "for place_folder in os.listdir(join(source, folders[1], subset)):\n",
    "    for file in os.listdir(join(source, folders[1], subset, place_folder)):\n",
    "        place_name = place_folder\n",
    "        image_number = '_'.join(file.split('_')[1:3])\n",
    "        frames.append(pd.DataFrame(\n",
    "            {\n",
    "                'image': join(source, folders[1], subset, place_folder,file),\n",
    "                'foggy_image': join(source, folders[2], subset, place_folder,f'{place_name}_{image_number}_leftImg8bit_foggy_beta_0.02.png'),\n",
    "                'target':join(source, folders[0], subset, place_folder, f'{place_name}_{image_number}_gtFine_labelIds.png')\n",
    "            },\n",
    "            index=[0]\n",
    "        ))\n",
    "\n",
    "df = pd.concat(frames).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "for i in range(len(df)):\n",
    "    images = df.iloc[i]\n",
    "    try:\n",
    "        im = Image.open(images.image)\n",
    "        im.verify()\n",
    "        im.close()\n",
    "    except:\n",
    "        print(images.image)\n",
    "    try:\n",
    "        im = Image.open(images.foggy_image)\n",
    "        im.verify()\n",
    "        im.close()\n",
    "    except:\n",
    "        print(images.foggy_image)\n",
    "    try:\n",
    "        im = Image.open(images.target)\n",
    "        im.verify()\n",
    "        im.close()\n",
    "    except:\n",
    "        print(images.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'dataset/cityscape_{subset}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import CLIPResNet, CLIPVisionTransformer\n",
    "import yaml\n",
    "import torch\n",
    "args = yaml.safe_load(open('trial_config.yaml', 'r'))\n",
    "\n",
    "resnet = CLIPResNet(\n",
    "        layers = args['Resnet']['layers'],\n",
    "        output_dim = args['Resnet']['output_dim'],\n",
    "        heads = args['Resnet']['heads'],\n",
    "        input_resolution = args['img_size'],\n",
    "        width = args['Resnet']['width'],\n",
    "        pretrained = args['Resnet']['pretrained']\n",
    "    )\n",
    "\n",
    "vit = CLIPVisionTransformer(\n",
    "        input_resolution = args['img_size'],\n",
    "        patch_size = args['VIT']['patch_size'],\n",
    "        width = args['VIT']['width'],\n",
    "        layers = args['VIT']['layers'],\n",
    "        heads = args['VIT']['heads'],\n",
    "        output_dim = args['VIT']['output_dim'],\n",
    "        drop_path_rate = args['VIT']['drop_path_rate'],\n",
    "        out_indices = args['VIT']['out_indices'],\n",
    "        get_embeddings = args['VIT']['get_embeddings'],\n",
    "        pretrained = args['VIT']['pretrained']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = resnet.get_features(torch.rand(1,3,512,512))\n",
    "for o in outs:\n",
    "    print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_outs = vit(torch.rand(1,3,512,512))\n",
    "vit_outs[0][0].shape, vit_outs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding out the classes in the cityscapes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import dataset, labels\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data = dataset('dataset/cityscape_train.csv', kaggle = False)\n",
    "\n",
    "image, foggy, gt = data.__getitem__(0)\n",
    "\n",
    "\n",
    "# gt = gt.permute(1,2,0)[:,:,:3]\n",
    "# gt_255 = (gt*255).to(torch.int)\n",
    "plt.imshow(image.permute(1,2,0))\n",
    "# plt.imshow(gt)\n",
    "# plt.show()\n",
    "# plt.imshow(gt_255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(34):\n",
    "    plt.imshow(gt[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(34,512,512)\n",
    "error = 0\n",
    "for i in range(512):\n",
    "    for j in range(512):\n",
    "        try:\n",
    "            color = tuple(gt[i,j].numpy())\n",
    "            id = color_mapping[color]\n",
    "            x[id,i,j] = 255\n",
    "        except:\n",
    "            x[0,i,j] = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "x1 = torch.randn(1,3,512,512).reshape(1,3,-1)\n",
    "x2 = torch.randn(1,3,512,512).reshape(1,3,-1)\n",
    "\n",
    "\n",
    "cos1 = nn.CosineSimilarity(dim=-1)\n",
    "(1-cos1(x1,x2)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.randn(1,3,512,512)\n",
    "\n",
    "x1.flatten(-2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos1(x1,x2).mean(),cos2(x1,x2).mean(),cos3(x1,x2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import dataset, labels\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data = dataset('dataset/cityscape_train.csv', image_size = 32, color_mapping = {label.color:label.id for label in labels}, kaggle = False)\n",
    "\n",
    "image, foggy, gt = data.__getitem__(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BCELoss()(gt,gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "label = torch.randint(low = 0, high = 2, size = (1,26,512,512))\n",
    "pred = torch.randint(low = 0, high = 2, size = (1,26,512,512))\n",
    "\n",
    "intersect = (pred & label).float().sum((2,3))\n",
    "union = (pred | label).float().sum((2,3))\n",
    "total_pixel_pred = pred.sum((2,3))\n",
    "total_pixel_truth = label.sum((2,3))\n",
    "\n",
    "(intersect/total_pixel_pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect = (pred & label).float().sum((2,3))\n",
    "sum = pred.sum((2,3)) + label.sum((2,3))\n",
    "dice = 2*intersect/sum \n",
    "dice>intersect/union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.rand(1,26,512,512)>0.5).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import dataset, labels\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data = dataset('dataset/cityscape_train.csv', image_size = 512, num_classes =20, kaggle = False)\n",
    "\n",
    "image, foggy, gt, mask = data.__getitem__(100)\n",
    "\n",
    "\n",
    "# for i in range(20):\n",
    "#     plt.imshow(gt[i])\n",
    "#     plt.title(i)\n",
    "#     plt.show()\n",
    "\n",
    "gt.unique(), gt.shape, mask[1:,:,:].unique(), mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    plt.imshow(mask[i,:,:])\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import labels\n",
    "class_names = ['void']\n",
    "for label in labels:\n",
    "    if not label.id == 0 and not label.ignoreInEval:\n",
    "        class_names.append(label.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import IoU\n",
    "\n",
    "iou = IoU()\n",
    "\n",
    "x1 = torch\n",
    "\n",
    "iou(torch.randn(2,2,512,512), torch.randn(2,2,512,512))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(2,20,512,512)\n",
    "\n",
    "x.softmax(dim=1).argmax(dim = 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Results from tb files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorboard as tb\n",
    "\n",
    "experiment = tb.data.experimental.ExperimentFromDev('Experiments/trial_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "'''\n",
    "This script exctracts training variables from all logs from \n",
    "tensorflow event files (\"event*\"), writes them to Pandas \n",
    "and finally stores in long-format to a CSV-file including\n",
    "all (readable) runs of the logging directory.\n",
    "\n",
    "The magic \"5\" infers there are only the following v.tags:\n",
    "[lr, loss, acc, val_loss, val_acc]\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Get all event* runs from logging_dir subdirectories\n",
    "logging_dir = './Experiments'\n",
    "event_paths = glob.glob(os.path.join(logging_dir, \"*\",\"event*\"))\n",
    "print(event_paths)\n",
    "\n",
    "\n",
    "# Extraction function\n",
    "def sum_log(path):\n",
    "    runlog = pd.DataFrame(columns=['metric', 'value'])\n",
    "    try:\n",
    "        for e in tf.train.summary_iterator(path):\n",
    "            for v in e.summary.value:\n",
    "                r = {'metric': v.tag, 'value':v.simple_value}\n",
    "                runlog = runlog.append(r, ignore_index=True)\n",
    "    \n",
    "    # Dirty catch of DataLossError\n",
    "    except:\n",
    "        print('Event file possibly corrupt: {}'.format(path))\n",
    "        return None\n",
    "    \n",
    "    print(runlog)\n",
    "\n",
    "    runlog['epoch'] = [item for sublist in [[i]*5 for i in range(0, len(runlog)//5)] for item in sublist]\n",
    "    \n",
    "    return runlog\n",
    "\n",
    "\n",
    "# Call & append\n",
    "all_log = pd.DataFrame()\n",
    "for path in event_paths:\n",
    "    log = sum_log(path)\n",
    "    if log is not None:\n",
    "        if all_log.shape[0] == 0:\n",
    "            all_log = log\n",
    "        else:\n",
    "            all_log = all_log.append(log)\n",
    "\n",
    "\n",
    "# Inspect\n",
    "print(all_log.shape)\n",
    "all_log.head()    \n",
    "            \n",
    "# Store\n",
    "all_log.to_csv('all_training_logs_in_one_file.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "ans = 0\n",
    "for e in summary_iterator('Experiments/trial_2/events.out.tfevents.1706374350.Sarthaks-MacBook-Pro.local.89088.0'):\n",
    "    ans += len(e.summary.value)\n",
    "    for v in e.summary.value:\n",
    "        print(v)\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
