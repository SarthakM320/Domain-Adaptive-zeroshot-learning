{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "model, preprocess = clip.load('ViT-B/16', device = f'cuda:{0}')\n",
    "\n",
    "\n",
    "img = Image.open('test_image.png')\n",
    "img = preprocess(img).unsqueeze(0).to(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.3037e-01, -4.3408e-01, -3.4033e-01,  3.0176e-01, -1.7664e-01,\n",
       "          3.3667e-01, -4.2236e-02, -6.1572e-01, -2.3572e-01,  6.5735e-02,\n",
       "         -4.5105e-02, -6.0645e-01,  2.1057e-02, -8.2336e-02,  1.3708e-01,\n",
       "         -1.8945e-01, -1.7236e-01,  2.0947e-01,  3.1616e-01, -6.5369e-02,\n",
       "         -3.0029e-01,  1.2866e-01,  2.1533e-01,  1.0046e-01,  2.1167e-01,\n",
       "          1.3074e-01, -2.5073e-01,  3.3813e-02,  7.4158e-02,  2.4695e-01,\n",
       "          3.6548e-01, -7.4585e-02,  2.6733e-01,  3.1647e-02,  2.5024e-01,\n",
       "         -2.3315e-01, -3.2898e-02, -3.2544e-01, -3.2275e-01, -3.3813e-01,\n",
       "         -3.4393e-02, -2.7490e-01, -1.1414e-01,  1.8188e-01, -3.4131e-01,\n",
       "         -3.8281e-01,  3.3740e-01, -1.2549e-01, -1.1627e-01,  1.5710e-01,\n",
       "         -3.7915e-01, -4.3286e-01,  2.4072e-01,  5.6549e-02, -2.2614e-02,\n",
       "         -3.0176e-01, -2.2437e-01,  4.4995e-01,  1.3953e-01,  2.9639e-01,\n",
       "          4.1656e-02,  2.6779e-02,  1.9226e-01,  2.4841e-01, -1.7407e-01,\n",
       "          5.8624e-02,  1.1847e-01, -2.3218e-01, -1.5967e-01, -2.4573e-01,\n",
       "         -1.5710e-01,  5.0439e-01, -1.5735e-01,  3.6328e-01,  5.6299e-01,\n",
       "          2.2925e-01, -1.9446e-01,  3.6890e-01,  3.3478e-02, -1.9373e-01,\n",
       "          7.1143e-01,  1.0381e+00,  1.8906e-02, -1.8689e-01, -3.6255e-01,\n",
       "          1.3574e-01,  4.4336e-01,  6.2622e-02,  3.0956e-03,  2.5244e-01,\n",
       "         -6.6528e-03, -4.8950e-02,  2.3132e-01, -2.0557e-01,  2.0398e-01,\n",
       "          1.4172e-01,  7.4316e-01,  1.6876e-02,  1.7261e-01, -3.0371e-01,\n",
       "          2.4536e-01, -1.1194e-01,  6.4880e-02, -1.7051e+00, -2.4878e-01,\n",
       "          2.4146e-01, -6.0333e-02, -9.7961e-03, -3.0273e-02,  2.3438e-01,\n",
       "          8.4961e-02, -1.8848e-01,  2.3889e-01,  2.6196e-01, -1.7078e-01,\n",
       "         -6.0333e-02,  5.8838e-01, -8.4473e-02, -2.3254e-01,  1.9238e-01,\n",
       "          1.2384e-01, -1.4233e-01, -1.1359e-01,  1.0199e-01,  8.7598e-01,\n",
       "          3.5400e-01,  8.5400e-01,  1.9104e-01,  1.4355e-01, -3.6841e-01,\n",
       "         -2.4707e-01, -1.3306e-01,  4.9019e-03,  3.0786e-01,  2.7417e-01,\n",
       "         -5.4443e-01,  1.0388e-01,  3.5327e-01,  1.3464e-01,  6.4575e-02,\n",
       "          1.7627e-01, -1.1407e-01, -2.8149e-01,  3.2422e-01, -2.3010e-01,\n",
       "          4.9469e-02,  5.4834e-01, -4.2944e-01, -9.2529e-02,  8.7952e-02,\n",
       "         -2.8491e-01,  8.3008e-02, -8.1421e-02, -2.1692e-01, -3.1421e-01,\n",
       "         -8.7341e-02, -1.7932e-01,  6.8726e-02,  4.8120e-01,  1.9629e-01,\n",
       "          4.2206e-02, -4.5868e-02, -2.8149e-01, -1.4844e-01,  1.0999e-01,\n",
       "         -2.6685e-01, -1.3481e-02, -2.6392e-01,  2.4792e-01, -3.0518e-01,\n",
       "         -4.7363e-02,  2.0361e-01,  3.0615e-01,  1.1475e-01, -3.3203e-01,\n",
       "         -2.7759e-01, -1.9629e-01,  3.9966e-01,  1.8057e+00,  1.7883e-01,\n",
       "          7.4768e-02,  4.7852e-01, -1.9434e-01, -1.3196e-01,  1.1481e-01,\n",
       "         -1.5723e-01, -3.2288e-02, -1.4209e-01, -5.4248e-01,  9.1003e-02,\n",
       "         -8.6853e-02, -4.8682e-01,  3.9282e-01, -9.9304e-02,  2.5439e-01,\n",
       "         -1.2494e-01, -5.5225e-01, -8.4778e-02,  5.2887e-02,  2.5864e-02,\n",
       "         -1.7859e-01,  9.9548e-02,  9.8694e-02, -3.9404e-01, -1.6431e-01,\n",
       "          1.5247e-01, -5.2686e-01, -3.5059e-01, -1.3000e-01, -3.5547e-01,\n",
       "          2.1069e-01,  2.5101e-02, -6.8787e-02, -3.2202e-01, -5.3467e-02,\n",
       "          9.1187e-02, -3.0518e-01, -3.0469e-01, -9.7595e-02,  1.5839e-02,\n",
       "         -1.2360e-01,  1.6602e-02,  3.3496e-01, -3.1836e-01, -4.9829e-01,\n",
       "         -8.7402e-02,  3.0664e-01, -1.8127e-01,  3.4351e-01, -2.7539e-01,\n",
       "         -4.0100e-02,  3.0347e-01, -1.7347e-03,  1.8677e-01,  2.7563e-01,\n",
       "          5.8887e-01, -6.5674e-02, -1.6165e-03,  1.9495e-01,  1.2024e-01,\n",
       "         -2.9590e-01, -2.6440e-01,  4.3188e-01,  9.6985e-02, -3.0566e-01,\n",
       "          8.0444e-02, -2.6978e-01,  1.0406e-01,  4.2480e-01, -3.4399e-01,\n",
       "         -1.2781e-01, -9.8242e-01, -2.7786e-02, -1.0736e-01,  1.0449e+00,\n",
       "          3.3447e-02,  1.2866e-01,  2.8320e-01,  3.9478e-01,  1.3550e-01,\n",
       "          7.9248e-01,  1.0114e-01, -3.7134e-01, -1.9446e-01, -4.9902e-01,\n",
       "          1.7981e-01,  7.2144e-02, -3.6835e-02, -2.6196e-01,  3.9185e-01,\n",
       "          5.8212e-03, -9.8190e-03, -1.8848e-01,  2.2583e-01, -5.7068e-02,\n",
       "         -2.6929e-01, -7.2021e-02,  3.3789e-01, -1.5637e-01,  4.2651e-01,\n",
       "         -5.8258e-02,  1.4758e-01, -1.0971e-02,  2.5195e-01,  1.4111e-01,\n",
       "          1.1444e-01,  9.5642e-02,  1.1407e-01,  1.3916e-01, -1.5088e-01,\n",
       "          1.0529e-01, -1.5332e-01, -6.1554e-02, -2.1133e-02, -7.2174e-03,\n",
       "          1.3464e-01, -3.0786e-01,  2.9443e-01,  3.3142e-02,  3.9307e-02,\n",
       "         -2.3523e-01,  6.6040e-02,  5.1849e-02,  2.3511e-01,  5.5371e-01,\n",
       "         -2.9688e-01,  5.1880e-02, -2.4158e-01,  4.7803e-01, -1.1798e-01,\n",
       "          3.2593e-01, -2.8735e-01, -2.5879e-01, -4.7388e-01, -2.9663e-01,\n",
       "         -9.2285e-02, -1.5732e-02, -4.6112e-02,  1.5466e-01, -2.9102e-01,\n",
       "         -3.7646e-01,  3.3521e-01, -1.3696e-01,  2.5244e-01, -1.6077e-01,\n",
       "         -2.3352e-01,  1.5881e-01, -1.7590e-01, -2.7710e-01, -1.2018e-01,\n",
       "         -1.2109e-01,  2.4390e-01, -1.0217e-01, -2.0105e-01, -1.4539e-01,\n",
       "          2.9861e-02, -4.7534e-01, -2.8760e-01, -5.0842e-02,  2.6489e-01,\n",
       "         -1.6968e-01,  1.1188e-01,  4.3286e-01, -8.6609e-02, -6.3354e-02,\n",
       "         -1.4258e-01,  1.7810e-01, -1.4473e-02, -3.8794e-01, -3.0127e-01,\n",
       "          1.6846e+00,  2.7710e-01, -1.9312e-01,  1.3708e-01, -2.6465e-01,\n",
       "          1.9348e-02,  3.8721e-01, -2.1228e-01, -2.0789e-01, -9.5825e-02,\n",
       "         -1.3818e-01,  2.5253e-02,  1.2607e+00, -3.6011e-01,  3.8354e-01,\n",
       "          2.9126e-01, -8.1299e-02,  3.8525e-01,  1.1578e-01,  2.9712e-01,\n",
       "          3.2373e-01, -1.8079e-01,  6.6833e-02,  2.9321e-01, -4.5166e-01,\n",
       "          1.2305e-01,  1.7236e-01, -3.4644e-01,  1.2535e-02, -2.4280e-01,\n",
       "         -1.3257e-01, -5.9424e-01,  7.7930e-01,  2.1582e-01,  8.7219e-02,\n",
       "          4.1138e-01,  1.7029e-01,  3.7329e-01, -2.5610e-01,  1.5503e-01,\n",
       "          8.7598e-01,  6.6223e-02,  2.0898e-01,  1.6260e-01, -3.8965e-01,\n",
       "         -1.1865e-01,  1.2195e-01,  2.5757e-01,  1.5588e-01, -2.1375e-01,\n",
       "          4.9103e-02, -1.9092e-01, -1.2421e-01,  1.2695e-02,  2.3145e-01,\n",
       "         -6.6223e-02, -1.4844e-01,  2.4139e-02,  1.6876e-02,  4.4067e-01,\n",
       "         -3.5034e-02,  8.3838e-01, -1.5381e-01, -3.1274e-01,  1.0048e-02,\n",
       "          1.7395e-01, -2.1667e-01,  4.5581e-01,  1.9116e-01,  6.4502e-01,\n",
       "          4.5410e-01, -3.0579e-02, -8.0627e-02,  2.7539e-01,  3.0078e-01,\n",
       "         -2.8198e-01, -1.2439e-01,  7.7576e-02, -6.1829e-02,  2.8369e-01,\n",
       "         -9.6436e-03, -4.1919e-01, -1.7468e-01,  2.4487e-01, -1.0736e-01,\n",
       "          4.0137e-01, -1.9507e-01, -2.5854e-01, -2.5562e-01, -2.5439e-01,\n",
       "          1.5869e-01,  4.9706e-03,  7.9414e+00, -2.6199e-02, -1.1859e-01,\n",
       "         -1.8689e-01,  2.9834e-01,  3.4985e-01, -2.4426e-01,  3.5645e-01,\n",
       "         -6.0791e-02,  2.6514e-01,  1.6138e-01, -2.0203e-01,  9.7107e-02,\n",
       "         -2.9102e-01,  2.5684e-01,  3.6060e-01, -2.3279e-01, -1.6406e-01,\n",
       "          3.5278e-01, -2.1851e-01,  1.7929e-02,  1.3863e-02, -1.1011e-01,\n",
       "         -5.5908e-01, -3.8599e-01,  1.1934e+00, -3.9062e-02,  6.8237e-02,\n",
       "         -2.6831e-01,  5.0568e-02,  1.1603e-01,  1.0468e-01, -3.4204e-01,\n",
       "         -8.6133e-01,  2.2324e-02, -1.4709e-01,  2.5220e-01, -2.1655e-01,\n",
       "          1.1298e-01, -9.9316e-01,  3.4424e-02, -2.1179e-01, -4.4580e-01,\n",
       "          4.1040e-01, -4.3848e-01, -2.8882e-01, -2.9004e-01, -1.8689e-01,\n",
       "          2.5787e-02,  2.0032e-01, -4.2664e-02, -1.5295e-01, -1.8945e-01,\n",
       "          3.8379e-01, -9.2712e-02, -1.2634e-01,  1.1230e-01, -2.4890e-01,\n",
       "         -4.4220e-02, -3.4082e-01, -2.9102e-01, -4.6631e-01, -1.8787e-01,\n",
       "          7.1924e-01, -7.8964e-03, -3.3252e-01, -1.7090e-02,  2.0056e-01,\n",
       "         -1.2433e-01,  4.0552e-01]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.visual(torch.rand(1,3,224,224).to(0).to(torch.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encode_image(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.visual(img.to(torch.float16)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "├─Conv2d: 1-1                                 [-1, 768, 14, 14]         589,824\n",
      "├─LayerNorm: 1-2                              [-1, 197, 768]            1,536\n",
      "├─Transformer: 1-3                            [-1, 2, 768]              --\n",
      "|    └─Sequential: 2-1                        [-1, 2, 768]              --\n",
      "|    |    └─ResidualAttentionBlock: 3-1       [-1, 2, 768]              7,087,872\n",
      "|    |    └─ResidualAttentionBlock: 3-2       [-1, 2, 768]              7,087,872\n",
      "|    |    └─ResidualAttentionBlock: 3-3       [-1, 2, 768]              7,087,872\n",
      "|    |    └─ResidualAttentionBlock: 3-4       [-1, 2, 768]              7,087,872\n",
      "|    |    └─ResidualAttentionBlock: 3-5       [-1, 2, 768]              7,087,872\n",
      "|    |    └─ResidualAttentionBlock: 3-6       [-1, 2, 768]              7,087,872\n",
      "|    |    └─ResidualAttentionBlock: 3-7       [-1, 2, 768]              7,087,872\n",
      "|    |    └─ResidualAttentionBlock: 3-8       [-1, 2, 768]              7,087,872\n",
      "|    |    └─ResidualAttentionBlock: 3-9       [-1, 2, 768]              7,087,872\n",
      "|    |    └─ResidualAttentionBlock: 3-10      [-1, 2, 768]              7,087,872\n",
      "|    |    └─ResidualAttentionBlock: 3-11      [-1, 2, 768]              7,087,872\n",
      "|    |    └─ResidualAttentionBlock: 3-12      [-1, 2, 768]              7,087,872\n",
      "├─LayerNorm: 1-4                              [-1, 768]                 1,536\n",
      "===============================================================================================\n",
      "Total params: 85,647,360\n",
      "Trainable params: 85,647,360\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 455.42\n",
      "===============================================================================================\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 2.59\n",
      "Params size (MB): 326.72\n",
      "Estimated Total Size (MB): 329.88\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "mod = model.visual.to(torch.float32)\n",
    "mod.proj = None\n",
    "summary(mod, (3,224,224));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feats = []\n",
    "def hook(module, input, output):\n",
    "    feats.append(output)\n",
    "\n",
    "# ids = [m[1].register_forward_hook(hook) for m in mod.named_modules()]\n",
    "\n",
    "id = mod.transformer.register_forward_hook(hook)\n",
    "_ = mod(torch.randn(2,3,224,224).to(0))\n",
    "id.remove()\n",
    "# for id in ids:\n",
    "#     id.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768, 14, 14])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = feats[0]\n",
    "\n",
    "out.transpose(0,1)[:,1:].transpose(1,2).reshape(2,768,14,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip \n",
    "import torch\n",
    "\n",
    "model, preprocess = clip.load('ViT-B/16', device = f'cuda:{0}')\n",
    "\n",
    "outs = model.visual(torch.rand(1,3,224,224).to(0).to(torch.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.forward(self, x: torch.Tensor)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.visual.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m outs\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "outs = model.visual.forward(torch.rand(1,3,224,224).to(0).to(torch.float16))\n",
    "outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encode_image(torch.rand(1,3,224,224).to(0).to(torch.float16)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 77])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(0)\n",
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encode_text(clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = []\n",
    "feats.append(model.encode_image(torch.rand(1,3,224,224).to(0).to(torch.float16)))\n",
    "feats.append(model.encode_text(clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing the forward function in Zegclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarth\\miniconda3\\envs\\zslearn\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512, 14, 14])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from models import CLIPViT\n",
    "import torch\n",
    "import math \n",
    "\n",
    "outs = CLIPViT().get_features(torch.rand(1,3,224,224).to(0).to(torch.float16),layers = [11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def d3_to_d4(t):\n",
    "    n, hw, c = t.size()\n",
    "    if hw % 2 != 0:\n",
    "        t = t[:, 1:]\n",
    "    h = w = int(math.sqrt(hw))\n",
    "    return t.transpose(1, 2).reshape(n, c, h, w)\n",
    "\n",
    "def d4_to_d3(t):\n",
    "    return t.flatten(-2).transpose(-1, -2)\n",
    "\n",
    "inputs_both = (outs, torch.rand(3,512).to(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([1, 512]), torch.Size([3, 512]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = inputs_both[0][0]\n",
    "cls_token = inputs_both[0][1]\n",
    "text_token = inputs_both[1]\n",
    "\n",
    "len(inputs), cls_token.shape, text_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 14, 14])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for stage_ in inputs[:1]:\n",
    "    x.append(d4_to_d3(stage_) if stage_.dim() > 3 else stage_)\n",
    "x.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1024])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_qs(q, cls):\n",
    "    # q = [q.cls, q]\n",
    "    C, dim = q.shape\n",
    "    bs, _ = cls.shape\n",
    "    q = q.expand(bs, -1, -1)\n",
    "    q1 = torch.einsum(\"bd,bcd->bcd\", cls, q)\n",
    "    q_ = torch.concat((q1, q), dim=-1)\n",
    "    return q_\n",
    "\n",
    "get_qs(text_token, cls_token).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_token.expand(1,-1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 14, 14])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarth\\miniconda3\\envs\\zslearn\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from models import SegDecoder\n",
    "import torch \n",
    "\n",
    "inputs_both = [\n",
    "    [(torch.rand(1,768,14,14).to(0),torch.rand(1,512,14,14).to(0)),torch.rand(1,512).to(0)],\n",
    "    torch.rand(3,512).to(0)\n",
    "]\n",
    "\n",
    "decoder = SegDecoder(224,768,[0,1],[0,1]).to(0)\n",
    "\n",
    "decoder(inputs_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "\n",
    "image_encoder = clip.load('RN50', device = f'cuda:{0}')[0].visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = dict(image_encoder.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarth\\miniconda3\\envs\\zslearn\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from models import CLIPResnet\n",
    "import torch\n",
    "\n",
    "feats = CLIPResnet().get_features(torch.rand(1,3,224,224).to(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 56, 56])\n",
      "torch.Size([1, 512, 28, 28])\n",
      "torch.Size([1, 1024, 14, 14])\n",
      "torch.Size([1, 2048, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "for f in feats:\n",
    "    print(f.shape)\n",
    "\n",
    "feats.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 224, 224])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import Decoder\n",
    "\n",
    "decoder = Decoder(10).to(0).to(torch.float16)\n",
    "\n",
    "decoder(feats).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import CLIPResNet, CLIPVisionTransformer\n",
    "import torch \n",
    "\n",
    "model = CLIPResNet(\n",
    "    layers=[3, 4, 6, 3],\n",
    "    output_dim=1024,\n",
    "    heads = 8,\n",
    "    input_resolution=512\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 128, 128])\n",
      "torch.Size([1, 512, 64, 64])\n",
      "torch.Size([1, 1024, 32, 32])\n",
      "torch.Size([1, 2048, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "outs = model.get_features(torch.rand(1,3,512,512))\n",
    "\n",
    "for o in outs:\n",
    "    print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import CLIPVisionTransformer\n",
    "import torch \n",
    "\n",
    "model = CLIPVisionTransformer(512,16,out_indices=[11,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "outs = model(torch.rand(1,3,512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('trial_config.yaml', 'r') as f:\n",
    "    file = yaml.safe_load(f)\n",
    "\n",
    "file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zslearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
